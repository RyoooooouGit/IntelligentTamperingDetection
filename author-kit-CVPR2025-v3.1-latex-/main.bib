@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
  author = {FirstName LastName},
  title  = {The frobnicatable foo filter},
  note   = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
  year   = 2014
}

@misc{Authors14b,
  author = {FirstName LastName},
  title  = {Frobnication tutorial},
  note   = {Supplied as supplemental material {\tt tr.pdf}},
  year   = 2014
}

@article{Alpher02,
  author  = {FirstName Alpher},
  title   = {Frobnication},
  journal = PAMI,
  volume  = 12,
  number  = 1,
  pages   = {234--778},
  year    = 2002
}

@article{Alpher03,
  author  = {FirstName Alpher and  FirstName Fotheringham-Smythe},
  title   = {Frobnication revisited},
  journal = {Journal of Foo},
  volume  = 13,
  number  = 1,
  pages   = {234--778},
  year    = 2003
}

@article{Alpher04,
  author  = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
  title   = {Can a machine frobnicate?},
  journal = {Journal of Foo},
  volume  = 14,
  number  = 1,
  pages   = {234--778},
  year    = 2004
}

@inproceedings{Alpher05,
  author    = {FirstName Alpher and FirstName Gamow},
  title     = {Can a computer frobnicate?},
  booktitle = CVPR,
  pages     = {234--778},
  year      = 2005
}

@misc{example2024,
  author = {John Doe},
  title  = {An Example Article on the Web},
  year   = {2024},
  url    = {https://www.example.com},
  note   = {Accessed: 2024-12-30}
}

@article{sym11101223,
  author         = {Wei, Xiaoyan and Wu, Yirong and Dong, Fangmin and Zhang, Jun and Sun, Shuifa},
  title          = {Developing an Image Manipulation Detection Algorithm Based on Edge Detection and Faster R-CNN},
  journal        = {Symmetry},
  volume         = {11},
  year           = {2019},
  number         = {10},
  article-number = {1223},
  url            = {https://www.mdpi.com/2073-8994/11/10/1223},
  issn           = {2073-8994},
  abstract       = {Due to the wide availability of the tools used to produce manipulated images, a large number of digital images have been tampered with in various media, such as newspapers and social networks, which makes the detection of tampered images particularly important. Therefore, an image manipulation detection algorithm leveraged by the Faster Region-based Convolutional Neural Network (Faster R-CNN) model combined with edge detection was proposed in this paper. In our algorithm, first, original tampered images and their detected edges were sent into symmetrical ResNet101 networks to extract tampering features. Then, these features were put into the Region of Interest (RoI) pooling layer. Instead of the RoI max pooling approach, the bilinear interpolation method was adopted to obtain the RoI region. After the RoI features of original input images and edge feature images were sent into bilinear pooling layer for feature fusion, tampering classification was performed in fully connection layer. Finally, Region Proposal Network (RPN) was used to locate forgery regions. Experimental results on three different image manipulation datasets show that our proposed algorithm can detect tampered images more effectively than other existing image manipulation detection algorithms.},
  doi            = {10.3390/sym11101223}
}

@article{SANDHYA2024301663,
  title    = {A novel method for real-time object-based copy-move tampering localization in videos using fine-tuned YOLO V8},
  journal  = {Forensic Science International: Digital Investigation},
  volume   = {48},
  pages    = {301663},
  year     = {2024},
  issn     = {2666-2817},
  doi      = {https://doi.org/10.1016/j.fsidi.2023.301663},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666281723001828},
  author   = { Sandhya and Abhishek Kashyap},
  keywords = {Copy-move tampering detection, YOLO V8, Passive technique, Object-based forgery, Tampered region localization, Spatio-temporal video forgery},
  abstract = {The research community faces challenges for video forgery detection techniques as advancements in multimedia technology have made it easy to alter the original video content and share it on electronic and social media with false propaganda. The copy-move attack is the most commonly practiced type of attack in videos/images, where an object is copied and moved into the current frame or any other frame of the video. Hence an illusion of recreation can be created to forge the content. It is very difficult to differentiate to uncover the forgery traces by the naked eye. Hence, a passive method-based algorithm is proposed to scientifically investigate the statistical properties of the video by normalizing the median difference of the frames at the pixel level, and graphical analysis successfully shows the clear peak in the forged region. After that, a new deep learning approach, “You Only Look at Once”, the latest eighth version of YOLO, is tuned and trained for the localization of forged objects in the real-time domain. The validation and testing results obtained from the trained YOLO V8 are successfully able to detect and localize the forged objects in the videos with mean average precision (mAP) of 0.99, recall is 0.99, precision is 0.99, and highest confidence score. The proposed YOLO V8 is fine-tuned in three different ways, and the performance of the proposed method outperforms existing state-of-the-art techniques in terms of inference speed, accuracy, precision, recall, testing, and training time.}
}

@article{18512,
  title   = {提高图像篡改检测区域选取性能的FCR-CNN模型},
  journal = {计算机辅助设计与图形学学报},
  volume  = {33},
  number  = {4},
  pages   = {560-568},
  year    = {2021},
  issn    = {1003-9775},
  doi     = {10.3724/SP.J.1089.2021.18512},
  url     = {https://www.jcad.cn/cn/article/doi/10.3724/SP.J.1089.2021.18512},
  author  = {魏晓燕 and 左鑫兰 and 但志平 and 吴义熔 and 董方敏 and 孙水发}
}

@article{LiuHaoyue2021RGBN,
  title   = {基于改进RGB-N的图像操纵检测算法},
  author  = {刘昊岳 Liu Haoyue and 马文伟 Ma Wenwei and 付晓 Fu Xiao and 沈程秀 Shen Chengxiu and 王亚领 Wang Yaling},
  journal = {Laser \& Optoelectronics Progress},
  year    = {2021},
  url     = {https://api.semanticscholar.org/CorpusID:244098310}
}

@article{article,
  author  = {Mallick, Devjani and Shaikh, Mantasha and Gulhane, Anuja and Maktum, Tabassum},
  year    = {2022},
  month   = {01},
  pages   = {03052},
  title   = {Copy Move and Splicing Image Forgery Detection using CNN},
  volume  = {44},
  journal = {ITM Web of Conferences},
  doi     = {10.1051/itmconf/20224403052}
}

@article{8977568,
  author   = {Rao, Yuan and Ni, Jiangqun and Zhao, Huimin},
  journal  = {IEEE Access},
  title    = {Deep Learning Local Descriptor for Image Splicing Detection and Localization},
  year     = {2020},
  volume   = {8},
  number   = {},
  pages    = {25611-25625},
  keywords = {Splicing;Feature extraction;Forgery;Deep learning;Kernel;Support vector machines;Convolutional neural networks;Image splicing detection;splicing localization;convolutional neural network;feature fusion;conditional random field (CRF)},
  doi      = {10.1109/ACCESS.2020.2970735}
}
